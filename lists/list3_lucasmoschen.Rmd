---
title: "Finanças Quantitativas: Lista 3"
author: "Lucas Moschen"
affiliation: "Fundação Getulio Vargas"
date: "03 de maio de 2020"
output: pdf_document
---

# Exercise 1

## Exercise 3.11 (Carmona)

*Let us assume that* $X \sim N(0,1)$ *and let us define the random variable* $Y$ *by:*
$$
Y = \frac{1}{\sqrt{1 - 2/\pi}}(\mid X\mid - \sqrt{2/\pi})
$$

1. $Compute~\mathbb{E}\{|X|\}$

\textbf{Resposta:}

Calculemos através da integral o valor esperado e considere $\phi(x)$ a função densidade da distribuição normal padrão.:

$$
\begin{split}
\mathbb{E}(|X|) &=  \int_{-\infty}^{\infty} |x|\phi(x)dx \\
&= \int_{-\infty}^{0} -x\phi(x)dx + \int_{0}^{\infty} x\phi(x)dx,~make~y = -x \\ 
&= \int_{0}^{\infty} y\phi(y)dy + \int_{0}^{\infty} x\phi(x)dx \\
&= 2\int_{0}^{\infty} x\phi(x)dx \\ 
&= 2\frac{1}{\sqrt{2\pi}}\int_0^{\infty} xe^{\frac{-x^2}{2}}dx, ~make~\frac{x^2}{2} = y\\
&= 2\frac{1}{\sqrt{2\pi}}\int_0^{\infty} e^{-y}dx \\
&= \sqrt{\frac{2}{\pi}}
\end{split}
$$
2. $Show~that~Y~has~mean~zero, ~variance~1,~and~that~it~is~uncorrelated~with~X$. 

\textbf{Resposta:}

Vejamos que $\mathbb{E}(Y) = \frac{1}{\sqrt{1 - 1/\pi}}(\mathbb{E}(|X|) - \sqrt{\frac{2}{\pi}}) = 0$. A primeira igualdade se deve a linearidade do valor esperado, enquanto a segunda igualdade se deve ao valor esperado calculado no item a. 

Vamos calcular $E[Y^2]$. Temos, então:

$$
\begin{split}
\mathbb{E}[Y^2] &= \frac{1}{1 - 2/\pi}(\mathbb{E}[X^2] - 2/\pi), ~linearidade~de~\mathbb{E}(\dot{}) \\
&= \frac{1}{1 - 2/\pi}(Var[X] + (\mathbb{E}[X])^2- 2/\pi) \\
&= \frac{1}{1 - 2/\pi}(1 - 2/\pi) = 1 \\
\end{split}
$$

Logo $Var[Y] = E[Y^2] - (E[Y])^2 = 1$. 

Por fim, calcularemos a covariância entre as variáveis: 

$$
\begin{split}
Cov(X,Y) &= \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y] = \mathbb{E}[XY] \\
&= \frac{1}{\sqrt{1 - 2/\pi}}\mathbb{E}(X|X|) - \sqrt{2/\pi}\mathbb{E}(X) \\
&= \frac{1}{\sqrt{1 - 2/\pi}}\left(\int_0^{\infty} x^2\phi(x)dx + \int_{-\infty}^0 (-x^2)\phi(x)dx\right),~make~y = -x \\
&= \frac{1}{\sqrt{1 - 2/\pi}}\left(\int_0^{\infty} x^2\phi(x)dx + \int_{0}^{\infty} (-y^2)\phi(y)dy\right) = 0
\end{split}
$$

Assim, mostramos que $\mathbb{E}[Y] = 1, Var[Y] = 0$ e $Cov(X,Y) = 0$. 

## Exercise 3.12 (Carmona)

*Let X and Y be continuous random variables with cdfs* $F_X$ *and* $F_Y$ *respectively, and with copula C. For each real number t, prove the following two equalities:*

1. $\mathbb{P}\{\max(X,Y) \leq t\} = C(F_X(t), F_Y(t))$

\textbf{Resposta:}

Seja $F_{X,Y}(x,y)$ a distribuição conjunta dessas variáveis. Pelo Teorema de Sklar's, temos que $F_{X,Y}(x,y) = C(F_X(x), F_Y(y))$. Além do mais, esta cópula é única, pois as variáveis são contínuas. Desta forma:

$$
F_{X,Y}(t,t) = \mathbb{P}[X \leq t, Y \leq t] = \mathbb{P}[\max(X,Y) \leq t]
$$
$$
F_{X,Y}(t,t) = C\left(F_X(t), F_Y(t)\right)
$$
Logo, temos que $\mathbb{P}\{\max(X,Y) \leq t\} = C(F_X(t), F_Y(t))$

2. $\mathbb{P}\{\min(X,Y) \leq t\} = F_X(t) + F_Y(t) - C(F_X(t), F_Y(t))$

\textbf{Resposta:}

Utilizando o teorema acima citado, podemos calcular: 

\begin{equation}
\begin{split}
\mathbb{P}\{\min(X,Y) \leq t\} &= 1 - \mathbb{P}\{\min(X,Y) \geq t\} \\
&= 1 - \mathbb{P}\{X \geq t, Y \geq t\} \\
&= 1 - (1 - \mathbb{P}\{X \leq t, Y\leq t\} - \mathbb{P}\{X \leq t, Y\geq t\} - \mathbb{P}\{X \geq t, Y\leq t\}) \\
&= \mathbb{P}\{X \leq t, Y\leq t\} + \mathbb{P}\{X \leq t, Y\geq t\} + \mathbb{P}\{X \geq t, Y\leq t\} \\
&= \mathbb{P}\{X \leq t, Y \in \mathbb{R}\} + \mathbb{P}\{X \geq t, Y\leq t\} + \mathbb{P}\{X \leq t, Y\leq t\} - \mathbb{P}\{X \leq t, Y\leq t\} \\
&= \mathbb{P}\{X \leq t, Y \in \mathbb{R}\} + \mathbb{P}\{X \in \mathbb{R}, Y\leq t\}  - \mathbb{P}\{X \leq t, Y\leq t\} \\
&= F_X(x) + F_Y(y) - C(F_X(t), F_Y(t)) 
\end{split}
\end{equation}

como queríamos mostrar. 

# Exercise 2 

*Let* $X = (X_1, X_2) \sim N_d(\mu, \Sigma)$, *where* $X_1 = (X_{1,1}, ..., X_{1,p}), X_2 = (X_{2,1}, ..., X_{2,q}), d = p + q$. *Show that*

|   a. $X_1 \sim N_p(\mu_1, \Sigma_{1,1})$.

\textbf{Resposta:}

Tenho que provar que $\forall (l_1, l_2, ..., l_p) \in \mathbb{R}^p, \xi = l_1X_{1,1} + ... + l_pX_{1,p}$ é Gaussiano. Suponha que não e que $(a_1, ...., a_p) \in \mathbb{R}^{p}$ seja tal que $a_1X_{1,1} + ... + a_pX_{1,p}$ não tem distribuição normal. Nesse caso, considere o vetor $d-dimensional~(a_1, ..., a_p, 0, 0, ...,0)$. Temos que, pela definição de Distribuição Normal Multivariada, $X_1$ tem distribuição normal e $a_1X_{1,1} + ... + a_pX_{1,p} + 0X_{2,1} + ... + 0X_{2,p}$ tem distribuição normal, o que é um absurdo. Desta forma, provamos por contradição que $X_1 \sim N_p(m, sd)$.

Sabemos que $m = (E[X_{1,1}], ..., E[X_{1,p}])$ e que $sd_{i,j} = Cov(X_{1,i}, X_{1,j})$. Sabemos, entretanto, que $m = \mu_1$, pois $E[X_1] = (E[X_{1,1}], ..., E[X_{1,p}]) = \mu_1$. Também sabemos que $sd_{i,j} = \Sigma_{1,1}(i,j)$, pois as definições são as mesmas quando $1 \leq i, j \leq p$. 

Concluimos, então,  que $X_1 \sim N_p(\mu_1, \Sigma_{1,1})$.

|   b. $X_1|X_2 = x_2 \sim N_p(\mu_{1|2}, \Sigma_{1|2})$, *where* $\mu_{1|2} = \mu_1 + \Sigma_{1,2}\Sigma_{2,2}^{-1}(x_2 - \mu_2)$ e $\Sigma_{1|2} = \Sigma_{1,1} - \Sigma_{1,2}\Sigma_{2,2}^{-1}\Sigma_{2,1}$. 

\textbf{Resposta:}

Suponha que $\Sigma$ é uma matriz invertível, considere $C = \Sigma_{1,2}\Sigma_{2,2}^{-1}$. Também considere as transformações $Y_1 = X_1 - CX_2$ e $Y_2 = X_2$. Dessa forma, teremos que $Y_1$ e $Y_2$ terão distribuição conjunta normal e matriz de covariância diagonal, que implica que elas serão independentes. Isso ajudará nas contas. Assim, primeiro vejamos que a distribuição conjunta é uma normal: 

\begin{equation}
\label{Yconj}
\begin{split}
\begin{pmatrix} Y_1 \\ Y_2 \end{pmatrix} &= \begin{pmatrix} I & -C \\ 0 & I \end{pmatrix}\begin{pmatrix}X_1 \\ X_2\end{pmatrix}, ~ pela~definicao \\
&\sim N_d\left(\begin{pmatrix}\mu_1 - C\mu_2 \\ \mu_2 \end{pmatrix}, \begin{pmatrix} I & -C \\ 0 & I \end{pmatrix} \begin{pmatrix} \Sigma_{1,1} & \Sigma_{1,2} \\ \Sigma_{2,1} & \Sigma_{2,2} \end{pmatrix}\begin{pmatrix} I & 0 \\ -C^T & I \end{pmatrix}\right) \\
&\sim N_d\left(\begin{pmatrix}\mu_1 - C\mu_2 \\ \mu_2 \end{pmatrix}, \begin{pmatrix} \Sigma_{1,1} - C\Sigma_{2,1} & \Sigma_{1,2} - C\Sigma_{2,2} \\ \Sigma_{2,1} & \Sigma_{2,2} \end{pmatrix}\begin{pmatrix} I & 0 \\ -C^T & I \end{pmatrix}\right) \\
&\sim N_d\left(\begin{pmatrix}\mu_1 - C\mu_2 \\ \mu_2 \end{pmatrix}, \begin{pmatrix} \Sigma_{1,1} - C\Sigma_{2,1} - \Sigma_{1,2}C^T +  C\Sigma_{2,2}C^T & \Sigma_{1,2} - C\Sigma_{2,2} \\ \Sigma_{2,1} - \Sigma_{2,2}C^T & \Sigma_{2,2} \end{pmatrix}\right) \\
\end{split}
\end{equation}

Desta forma, $Y_1$ e $Y_2$ tem distrivuição conjunta, com média e matriz de variância especificada em \ref{Yconj}. Sabemos que $\Sigma$ é uma matriz onde $\Sigma_{1,2} = \Sigma{2,1}^T$, visto quo que $Cov(X_{1,i}, X_{2,j}) = Cov(X_{2,j}, X_{1,i})$. Também sabemos que $\Sigma_{i,i} = \Sigma_{i,i}^T, i = 1, 2$, isto é, é simétrica, pelo mesmo motivo. Desta forma: 

\begin{equation}
\label{Yind}
\begin{split}
\begin{pmatrix} Y_1 \\ Y_2 \end{pmatrix} &\sim N_d\left(\begin{pmatrix}\mu_1 - C\mu_2 \\ \mu_2 \end{pmatrix}, \begin{pmatrix} \Sigma_{1,1} - C\Sigma_{2,1} - \Sigma_{1,2}C^T +  C\Sigma_{2,2}C^T & \Sigma_{1,2} - C\Sigma_{2,2} \\ \Sigma_{2,1} - \Sigma_{2,2}C^T & \Sigma_{2,2} \end{pmatrix}\right) \\
&\sim N_d\left(\begin{pmatrix}\mu_1 - C\mu_2 \\ \mu_2 \end{pmatrix}, \right. \\ &\left. \begin{pmatrix} \Sigma_{1,1} - (\Sigma_{1,2}\Sigma_{2,2}^{-1})\Sigma_{2,1} - \Sigma_{1,2}(\Sigma_{1,2}\Sigma_{2,2}^{-1})^T +  (\Sigma_{1,2}\Sigma_{2,2}^{-1})\Sigma_{2,2}(\Sigma_{1,2}\Sigma_{2,2}^{-1})^T & \Sigma_{1,2} - (\Sigma_{1,2}\Sigma_{2,2}^{-1})\Sigma_{2,2} \\ \Sigma_{2,1} - \Sigma_{2,2}(\Sigma_{1,2}\Sigma_{2,2}^{-1})^T & \Sigma_{2,2} \end{pmatrix}\right) \\
&\sim N_d\left(\begin{pmatrix}\mu_1 - C\mu_2 \\ \mu_2 \end{pmatrix}, \begin{pmatrix} \Sigma_{1,1} - (\Sigma_{1,2}\Sigma_{2,2}^{-1})\Sigma_{2,1} - \left[\Sigma_{1,2} +  \Sigma_{1,2}(\Sigma_{2,2}^{-1}\Sigma_{2,2})\right]
(\Sigma_{1,2}\Sigma_{2,2}^{-1})^T & \Sigma_{1,2} - \Sigma_{1,2}(\Sigma_{2,2}^{-1}\Sigma_{2,2}) \\ \Sigma_{2,1} - (\Sigma_{2,2}^T(\Sigma_{2,2}^{-1})^T)\Sigma_{1,2}^T & \Sigma_{2,2} \end{pmatrix}\right) \\
&\sim N_d\left(\begin{pmatrix}\mu_1 - C\mu_2 \\ \mu_2 \end{pmatrix}, \begin{pmatrix} \Sigma_{1,1} - (\Sigma_{1,2}\Sigma_{2,2}^{-1})\Sigma_{2,1} - \left[\Sigma_{1,2} +  \Sigma_{1,2}(\Sigma_{2,2}^{-1}\Sigma_{2,2})\right] (\Sigma_{1,2}\Sigma_{2,2}^{-1})^T & \Sigma_{1,2} - \Sigma_{1,2}(\Sigma_{2,2}^{-1}\Sigma_{2,2}) \\ \Sigma_{2,1} - (\Sigma_{2,2}^T(\Sigma_{2,2}^{-1})^T)\Sigma_{1,2}^T & \Sigma_{2,2} \end{pmatrix}\right) \\
&\sim N_d\left(\begin{pmatrix}\mu_1 - C\mu_2 \\ \mu_2 \end{pmatrix}, \begin{pmatrix} \Sigma_{1,1} - (\Sigma_{1,2}\Sigma_{2,2}^{-1})\Sigma_{2,1} & 0 \\ 0 & \Sigma_{2,2} \end{pmatrix}\right)
\end{split}
\end{equation}

Com isso, provamos que $Y_1$ e $Y_2$ são independentes, isto é $X_1 - CX_2$ é independente de $X_2$. Isso é interessante, porque a distribuição condicional de $Y_1|Y_2 = y_2$, isto é $X_1 - CX_2|X_2 = x_2$ tem a mesma distribuição de $X_1 - Cx_2$. Mas pelos cálculos em \ref{Yind} e o resultado obtido na primeira questão, inferimos que $X_1 - CX_2 \sim N_p(\mu_1 - C\mu_2, \Sigma_{1,1} - C\Sigma_{2,1} )$. 

Concluimos que:

\begin{equation}
\begin{split}
X_1|X_2 = x_2 &\sim N_p(\mu_1 - C\mu_2 + Cx_2, \Sigma_{1,1} - C\Sigma_{2,1}) \\
&\sim N_p(\mu_1 + \Sigma_{1,2}\Sigma_{2,2}^{-1}(x_2 - \mu_2), \Sigma_{1,1} - \Sigma_{1,2}\Sigma_{2,2}^{-1}\Sigma_{2,1})
\end{split}
\end{equation}

como queríamos provar.

# Exercise 3 

*Let* $X \sim Exp(r), ~i.e~ f_X(x) = re^{-rx}~for~x > 0~and~Y \sim Pareto(\alpha), i.e., f_Y(y) = \left(1 + \frac{y}{\alpha}\right)^{-(1 + \alpha)}, ~for~y > 0$. *Couple these two random variables with Gumpel copula with parameter* $\theta$: 

$$C_{G_u}(u,v | \theta) = exp\{-((-\ln u)^{\theta} + (-\ln v)^{\theta})^{1/\theta}\}, 1 \leq \theta$$

|    a. *Derive the joint cdf of* $X$ *and* $Y$. 

\textbf{Resposta:}

Pelo Teorema de Sklar's, como $F_X$ e $F_Y$ são distribuições univariadas, temos que $F_{X,Y}$ é dada pela expressão:

\begin{equation}
\begin{split}
F_{X,Y}(x,y\mid \theta) &= C(F_X(x), F_Y(y)\mid\theta) \\
&= exp\{-[(-\ln F_X(x))^{\theta} + (-\ln F_Y(y))^{\theta}]^{1/\theta}\} \\
&= exp\left\{-\left[(-\ln (1 - e^{-rx}))^{\theta} + \left(-\ln\left(1 - \left(1 + \frac{y}{\alpha}\right)^{-\alpha}\right)\right)^{\theta}\right]^{1/\theta}\right\} \\
\end{split}
\end{equation}

|    b. *What happens to the dependence structure when* $\theta = 1$? *And when* $\theta \to \infty$. 

\textbf{Resposta:}

Se $\theta = 1$, observamos que $F_{X,Y}(x,y\mid\theta = 1)$:

\begin{equation}
\begin{split}
F_{X,Y}(x,y) &= exp\left\{-\left[(-\ln (1 - e^{-rx})) + \left(-\ln\left(1 - \left(1 + \frac{y}{\alpha}\right)^{-\alpha}\right)\right)\right]\right\} \\
&= exp\left\{\ln (1 - e^{-rx})\right\}exp\left\{\ln\left(1 - \left(1 + \frac{y}{\alpha}\right)^{-\alpha}\right)\right\} \\
&= (1 - e^{-rx})\left(1 - \left(1 + \frac{y}{\alpha}\right)^{-\alpha}\right) \\
&= F_X(x)F_Y(y)
\end{split} 
\end{equation}

Como isso ocorre independente de $x$ e $y$, temos que as variáveis são independentes, isto é, a estrutura de dependência é inexistente. No próximo caso, observaremos um contraponto a esse fato. 

Se $\theta \to \infty$, observamos que tanto $-\ln F_X(x)$ quanto $-\ln F_Y(y)$ são sempre positivos, visto que a distribuição cumulativa tem imagem em $[0,1]$. Dessa forma, podemos aplicar o conhecimento de que $\lim_{\theta \to \infty} ||x||_p = ||x||_{\infty}$. Também utilizo que a função logaritmo é crescente, logo, se em um dado ponto, o logaritmo de uma função é menor do que o logaritmo de outra, então esse ponto será menor também. Desta forma: 

$$
\begin{split}
\lim_{\theta\to\infty} F_{X,Y}(x,y) &= \lim_{\theta\to\infty} exp\left\{-\left[(-\ln (1 - e^{-rx}))^{\theta} + \left(-\ln\left(1 - \left(1 + \frac{y}{\alpha}\right)^{-\alpha}\right)\right)^{\theta}\right]^{1/\theta}\right\} \\
&= exp\left\{-\lim_{\theta\to\infty} \left[(-\ln (1 - e^{-rx}))^{\theta} + \left(-\ln\left(1 - \left(1 + \frac{y}{\alpha}\right)^{-\alpha}\right)\right)^{\theta}\right]^{1/\theta} \right\} \\
&= exp\left\{-\max\left[(-\ln (1 - e^{-rx})), \left(-\ln\left(1 - \left(1 + \frac{y}{\alpha}\right)^{-\alpha}\right)\right)\right]\right\} \\
&= exp\left\{\min\left[\ln (1 - e^{-rx}), \ln\left(1 - \left(1 + \frac{y}{\alpha}\right)^{-\alpha}\right)\right]\right\} \\
&= exp\left\{\ln(\min\left[1 - e^{-rx}, 1 - \left(1 + \frac{y}{\alpha}\right)^{-\alpha}\right])\right\} \\
&= \min\left[1 - e^{-rx}, 1 - \left(1 + \frac{y}{\alpha}\right)^{-\alpha}\right] \\
&> (1 - e^{-rx})(1 - \left(1 + \frac{y}{\alpha}\right)^{-\alpha})
\end{split} 
$$

A última desigualdade acontece porque nenhuma distribuição atinge os valores extremos do intervalo $[0,1]$, desta forma, o menor desses valores multiplicado por o maior que é menor do que um obtém um resultado menos. Desta forma, concluímos que $F_{X,Y}(x,y) \neq F_X(x)F_Y(y)$ e as variáveis \textbf{não} são independentes, pela definição de independência. Além do mais, a dependência é conectada com a comparação dos valores das duas ditribuições em cada ponto, isto é, se em uma parte da curva uma distribuição for mais que a outra, a distribuição conjunta será a menor.  

Desta maneira, concluímos que no primeiro caso há independência, enquanto no segundo, não há independência. 

|    c. *Generate data from the joint distribution of* $X_1$ *and* $X_2$. 

\textbf{Resposta:}

Primeiro, vamos importar a biblioteca e considerar o caso onde $\theta = 1$, para observar o comportamento da independência.

```{r message = FALSE}
library(copula)
if(require(actuar)==0){
  install.packages("actuar")
}
library(actuar)

theta = 1
C_gumbel = gumbelCopula(param = theta)

par(mar=c(2,4,1,2), mfrow=c(1,2))
persp(C_gumbel, dCopula, main ="Density", lwd=1)
contour(C_gumbel, dCopula, xlim = c(0, 1), ylim=c(0, 1), main = "Contour plot")
```

Agora, vamos gerar os dados: 

```{r message=FALSE}
r = 1
alpha = 1
multivariate_dist = mvdc(copula = C_gumbel,
                         margins = c("exp", "pareto"),
                         paramMargins = list(list(rate = r),
                                        list(shape = alpha, scale = 1)))
gen_data = rMvdc(n = 1000, mvdc = multivariate_dist)

theta = 5
C_gumbel2 = gumbelCopula(param = theta)
r = 3
alpha = 1.5 
multivariate_dist2 = mvdc(copula = C_gumbel2,
                         margins = c("exp", "pareto"),
                         paramMargins = list(list(rate = r),
                                        list(shape = alpha, scale = 1)))
gen_data2 = rMvdc(n = 1000, mvdc = multivariate_dist2)

plot(gen_data[,1], gen_data[,2], xlab = "X ~ Exp(r)", ylab = "Y ~ Pareto(a)",
     main = "Dados gerados pela distribuição conjunta", col = "green")
points(gen_data2[,1], gen_data2[,2], col = "blue")
legend("topright", legend = c("r = a = theta = 1", "r = 3, a = 1.5, theta = 5"), 
       fill = c("green", "blue"), cex = 0.8)

```

