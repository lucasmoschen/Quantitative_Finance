---
title: "Finanças Quantitativas: Lista 4"
author: "Lucas Moschen"
affiliation: "Fundação Getulio Vargas"
date: "08 de maio de 2020"
output: pdf_document
---

## Problema 3.17

1. Sejam $Z \sim N(0,1)$, $\sigma > 0$ e $f$ uma função. Então: 
$$
\begin{split}
\mathbb{E}[f(Z)e^{\sigma Z}] &= \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}f(z)e^{\sigma z} e^{-\frac{z^2}{2}} dz \\
&= \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}f(z) e^{-\frac{z^2}{2} + \sigma z} dz \\
&= \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}f(z) e^{-\frac{1}{2}(z^2 - 2\sigma z + \sigma^2)}e^{\frac{1}{2}\sigma^2} dz \\
&= \frac{1}{\sqrt{2\pi}}e^{\frac{1}{2}\sigma^2}\int_{-\infty}^{\infty}f(z) e^{-\frac{1}{2}(z - \sigma)^2} dz, ~~~x = z - \sigma, dx = dz  \\
&= \frac{1}{\sqrt{2\pi}}e^{\frac{1}{2}\sigma^2}\int_{-\infty}^{\infty}f(x + \sigma) e^{-\frac{1}{2}x^2} dx \\
&= e^{\frac{1}{2}\sigma^2}\mathbb{E}[f(Z + \sigma)] \\
\end{split}
$$
Se $X \sim N(\mu, \sigma^2)$, temos que $X = \mu + \sigma Z$, onde $Z \sim N(0,1)$. Se tomarmos $f(Z) = e^{\mu}$ e $sigma > 0$ o desvio vadrão de $X$. Assim: 
$$
\begin{split}
\mathbb{E}[e^X] &= \mathbb{E}[e^{\mu + \sigma Z}] \\
&= \mathbb{E}[e^{\mu}e^{\sigma Z}], ~pelo~demonstrado \\
&= e^{\frac{1}{2}\sigma^2}\mathbb{E}[f(Z + \sigma)] \\
&= e^{\frac{1}{2}\sigma^2}\mathbb{E}[e^{\mu}] = e^{\mu + \sigma^2/2} \\
\end{split}
$$
2. Assuma que $X$ e $Y$ são v.a. juntamente Gaussianas com média $0$ e $h$  uma função qualquer.

Temos que $f_{XY}(x,y)=\frac{1}{2 \pi \sigma_X \sigma_Y \sqrt{1-\rho^2}} \exp \{-\frac{1}{2 (1-\rho^2)} \big[ \frac{x^2}{\sigma_X^2}-2\rho\frac{xy}{\sigma_X\sigma_Y} +\frac{y^2}{\sigma_Y^2} \big] \}$
$$
\begin{split}
\mathbb{E}[e^Xh(Y)] &= \int_{\mathbb{R}^2} e^xh(y)f_{XY}(x,y)dydx \\
&= \int_{\mathbb{R}} e^x\frac{1}{\sqrt{2\pi}\sigma_X}\exp\{-\frac{x^2}{2\sigma_X^2}\} \int_{\mathbb{R}} h(y)\frac{1}{\sqrt{2 \pi} \sigma_Y \sqrt{1-\rho^2}}\exp\{\frac{x^2}{2\sigma_X^2}\} \exp \{-\frac{1}{2 (1-\rho^2)} \big[ \frac{x^2}{\sigma_X^2}-2\rho\frac{xy}{\sigma_X\sigma_Y} +\frac{y^2}{\sigma_Y^2} \big] \} dy dx \\
&= \int_{\mathbb{R}} e^x\frac{1}{\sqrt{2\pi}\sigma_X}\exp\{-\frac{x^2}{2\sigma_X^2}\} \int_{\mathbb{R}} h(y)\frac{1}{\sqrt{2 \pi} \sigma_Y \sqrt{1-\rho^2}}\exp\{ -\frac{1}{2 (1-\rho^2)} \big[\frac{x^2\rho^2}{\sigma_X^2} -2\rho\frac{xy}{\sigma_X\sigma_Y} +\frac{y^2}{\sigma_Y^2} \big] \} dy dx \\
&= \int_{\mathbb{R}} e^x\frac{1}{\sqrt{2\pi}\sigma_X}\exp\{-\frac{x^2}{2\sigma_X^2}\} \int_{\mathbb{R}} h(y)\frac{1}{\sqrt{1-\rho^2}\sqrt{2 \pi} \sigma_Y }\exp\{ -\frac{1}{2 (1-\rho^2)} \left(\frac{y - \frac{x\rho\sigma_Y}{\sigma_X}}{\sigma_Y}\right)^2\} dy dx \\
&= \mathbb{E}[e^X]\mathbb{E}[h(Y + cov(X,Y))] 
\end{split}
$$

\pagebreak 

## Problema 3.18

1. $\log X \sim N(0, 1)$ e $\log X \sim N(0, \sigma^2)$. Além, $(\log X, \log Y)$ são juntamente Gaussianas. Compute a denside de $X$ quando $\log X \sim N(\mu, \sigma^2)$

Seja $Y = \log X$. Considere $g(y) = e^y$, então $g$ transforma a variável $Y$ na variável $X$. Desta forma, consigo calcular a densidade de $X$: 
$$
\begin{split}
f_X(x) &= \frac{f_Y(y)}{g'(y)} = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y - \mu)^2}{\sigma^2}}\frac{1}{e^y} \\
&= \frac{1}{\sqrt{2\pi}\sigma}\exp\{-\frac{(\log x - \mu)^2}{\sigma^2} - \log x\}
\end{split}
$$
Temos, então, a densidade de $X$.

2. $\rho_{\min} = (e^{-\sigma} - 1)/\sqrt{(e - 1)(e^{\sigma^2} - 1)}$

Pelo descrito no exercício anterior, sabemos que: 

$$E[X] = E[e^{\log X}] = e^{\mu + \sigma^2/2} = e^{1/2}$$
$$E[Y] = e^{\sigma^2/2}$$
$$E[X^2] = E[e^{2\log X}] = e^{2\mu + 2\sigma^2} = e^2$$
$$E[Y^2] = e^{2\sigma^2}$$
$$Var[X] = E[X^2] - E[X]^2 = e^2 - e$$
$$Var[Y] = E[Y^2] - E[Y]^2 = e^{\sigma^2}(e^{\sigma^2} - 1)$$

Podemos dizer que $X$ e $Y$ são contramonotônicas, pois $(X, Y) = (e^{\log X})(e^{-\log Y}) = (e^{\log X})(e^{-\sigma\log X})$, isto é, existe uma variável aleatória e duas funções, uma crescente e uma deecrescente, onde vale essa igualdade em distribuição. Logo, elas admitem o limite inferior Fréchet-Hoeffding na cópula, que no caso é de uma Gaussiana. Assim, o valor mínimo da correlação é atingido quando a cópula é minima, que ocorre quando as variáveis são contramonotônicas. 

Agora, podemos calcular os valores desejados: 
$$
\begin{split}
\rho_{\min} &= \min \frac{Cor(X,Y)}{\sigma_X\sigma_Y} 
= \frac{Cov(e^{\log X}, e^{-\sigma\log X})}{\sigma_X\sigma_Y} \\
&= \frac{E[e^{\log X - \sigma\log X}] - e^{1/2 + \sigma^2/2}}{\sqrt{e(e - 1)}\sqrt{e^{\sigma^2}(e^{\sigma^2} - 1)}} 
= \frac{e^{(-\sigma + 1)^2/2} - e^{(\sigma^2 + 1)/2}}{e^{1/2 + \sigma^2/2}\sqrt{(e - 1)(e^{\sigma^2} - 1)}} \\
&= \frac{e^{(-\sigma + 1)^2/2 - (\sigma^2 + 1)/2} - 1}{\sqrt{(e - 1)(e^{\sigma^2} - 1)}} 
= \frac{e^{-\sigma} - 1}{\sqrt{(e - 1)(e^{\sigma^2} - 1)}}
\end{split}
$$
Como queríamos mostrar. 

3. $\rho_{\max} = (e^{\sigma} - 1)/\sqrt{(e - 1)(e^{\sigma^2} - 1)}$

De forma equivalente, podemos dizer que $X$ e $Y$ são comonotônicas, porque $(X,Y) = (e^{\log X}, e^{\sigma \log X})$ e temos duas funções crescentes com uma variável em comum. Desta forma, elas admitem o limite superior de Fréchet-Hoeffding. DEsta forma, maximizamos a covariância no valor máximo que a cópula assume. Utilizamos as contas do exercício anterior (3.17) e, da mesma forma que fizemos em 2. dessa exercício, a correlação máxima acontece quando as variáveis são comonotônicas. Logo:
$$
\begin{split}
\rho_{\max} &= \max \frac{Cor(X,Y)}{\sigma_X\sigma_Y} 
= \frac{Cov(e^{\log X}, e^{\sigma\log X})}{\sigma_X\sigma_Y} \\
&= \frac{E[e^{\log X + \sigma\log X}] - e^{1/2 + \sigma^2/2}}{\sqrt{e(e - 1)}\sqrt{e^{\sigma^2}(e^{\sigma^2} - 1)}} 
= \frac{e^{(\sigma + 1)^2/2} - e^{(\sigma^2 + 1)/2}}{e^{1/2 + \sigma^2/2}\sqrt{(e - 1)(e^{\sigma^2} - 1)}} \\
&= \frac{e^{(\sigma + 1)^2/2 - (\sigma^2 + 1)/2} - 1}{\sqrt{(e - 1)(e^{\sigma^2} - 1)}} 
= \frac{e^{\sigma} - 1}{\sqrt{(e - 1)(e^{\sigma^2} - 1)}}
\end{split}
$$
Como queríamos mostrar. 

4. $\lim_{\sigma\to\infty} \rho_{\min} = \lim_{\sigma\to\infty} \rho_{\max} = 0$

Como $\sigma > 0$, vemos que $\lim_{\sigma \to \infty} e^{-\sigma} = 0$, e que $e^{\sigma^2} \to \infty$. Dessa maneira, pelo denominador ser ilimitado e o numerador limitado, temos que, de fato, $\lim_{\sigma\to\infty} \rho_{\min} = 0$. 

Para conferir que $\lim_{\sigma\to\infty} \rho_{\max} = 0$, vemos que:

$$
\begin{split}
0 \leq \frac{e^{\sigma} - 1}{((e - 1)(e^{\sigma^2} - 1))^\frac{1}{2}} &\leq \frac{e^{\sigma}}{(e^{\sigma^2} - 1)^\frac{1}{2}}  \\
&= \frac{1}{(e^{\sigma^2} - 1)^\frac{1}{2}/(e^{2\sigma})^{\frac{1}{2}}} \\
&= \frac{1}{(e^{\sigma^2 - 2\sigma} - e^{-2\sigma})^\frac{1}{2}} \\
&\to 0
\end{split}
$$
Isso ocorre porque $e^{\sigma^2 - 2\sigma} \to \infty$, logo o denominador é ilimitado e o numerador limitado. 

Concluo que $\lim_{\sigma\to\infty} \rho_{\min} = \lim_{\sigma\to\infty} \rho_{\max} = 0$.

\pagebreak

## Problema 3.24

1. 

```{r message = FALSE}
library(copula)   #carregando a copula
rho <- seq(from = 0, to = 1, by = 0.05)

correlation <- list("Pearson" = c(), "Kendall" = c(), "Spearman" = c())
for (param in rho){
  ncopula <- normalCopula(param = param)
  SD <- rCopula(ncopula, n = 2000)
  # Exercise 1.1
  SD <- qnorm(SD, mean = 0, sd = 1) 
  # Exercise 1.2
  pcov <- cor(x = SD[,1], y = SD[,2], method = "pearson")
  kcov <- cor(x = SD[,1], y = SD[,2], method = "kendall")
  scov <- cor(x = SD[,1], y = SD[,2], method = "spearman")
  
  correlation$Pearson <- append(correlation$Pearson, pcov)
  correlation$Kendall <- append(correlation$Kendall, kcov)
  correlation$Spearman <- append(correlation$Spearman, scov)
}
print(dim(SD))

# Exercise 1.3 and 1.4

par(mfrow = c(3,1), mar = c(1,2,3,2))
plot(rho, correlation$Pearson, main = "Pearson Correlation",
     col = "red", ylab = "Cor. value" )
points(x = rho, y =  rho, type= 'l', col = "grey")
plot(rho, correlation$Kendall, main = "Kendall Correlation",
     col = "blue", ylab = "Cor. value")
points(x = rho, y =  rho, type= 'l', col = "grey")
plot(rho, correlation$Spearman, main = "Spearman Correlation", 
     col = "green", ylab = "Cor. value")
points(x = rho, y =  rho, type= 'l', col = "grey")
```

Observamos que nos gráficos de Pearson e Spearman, a correlação das variáveis comparada com a correlação do parâmetro da cópula Gaussiana está em cima da reta $y = x$. Dessa maneira, podemos, pelo menos visualmente, ver que o parâmetro da cópula é exatamente a correlação das variáveis que estamos modelando. Isso nos dá uma interpretação bem clara do que é esse parâmetro ao fazer amostras aleatórias.

No caso da correlação de Kendall, vemos que o gráfico fica um pouco a baixo da reta $y = x$, significando que ela é menor do que a correlação de Pearson para esses valores. Isso faz sentido por que a contribuição para uma mudança de sinal é equivalente sempre, enquanto na de Pearson, ela é Ponderada por seu tamanho.

```{r message = FALSE}
# Exercise 1.5: Cauchy Distribution
rho <- seq(from = 0, to = 1, by = 0.05)

correlation <- list("Pearson" = c(), "Kendall" = c(), "Spearman" = c())
for (param in rho){
  ncopula <- normalCopula(param = param)
  SD <- rCopula(ncopula, n = 2000)
  SD <- qcauchy(SD, location = 0, scale = 1) 
  pcov <- cor(x = SD[,1], y = SD[,2], method = "pearson")
  kcov <- cor(x = SD[,1], y = SD[,2], method = "kendall")
  scov <- cor(x = SD[,1], y = SD[,2], method = "spearman")
  
  correlation$Pearson <- append(correlation$Pearson, pcov)
  correlation$Kendall <- append(correlation$Kendall, kcov)
  correlation$Spearman <- append(correlation$Spearman, scov)
}

par(mfrow = c(3,1), mar = c(1,2,3,2))
plot(rho, correlation$Pearson, main = "Pearson Correlation",
     col = "red", ylab = "Cor. value" )
points(x = rho, y =  rho, type= 'l', col = "grey")
plot(rho, correlation$Kendall, main = "Kendall Correlation",
     col = "blue", ylab = "Cor. value")
points(x = rho, y =  rho, type= 'l', col = "grey")
plot(rho, correlation$Spearman, main = "Spearman Correlation", 
     col = "green", ylab = "Cor. value")
points(x = rho, y =  rho, type= 'l', col = "grey")
```

A correlação de Pearson ficou extremamente diferente. Isso faz com que o parâmetro não deva ser interpretado como a correlação das variáveis, dado que a cauda das distribuições marginais é mais pesada. A correlação de Kendall manteve a propriedade, dado que, como comentado anteriormente, o que é considerado é apenas o sinal, não o tamanho da diferença. Por fim, a correlação de Spearman mostrou-se ainda ter a mesma interpretação. Isso ocorre, pois ela tem o objetivo de remover o tamanho relativo de $X$ e $Y$ e captura a correlação apenas com o que sobrou da dependência, que nesse caso é uma cópula normal, que como visto acima, tem a mesma interpretação para Pearson e Spearman. 

2. 

```{r message = FALSE}
beta <- seq(from = 1, to = 20, by = 0.5)

correlation <- list("Pearson" = c(), "Kendall" = c(), "Spearman" = c())
for (param in beta){
  gcopula <- gumbelCopula(param = param)
  SD <- rCopula(gcopula, n = 2000)
  # Exercise 2.1
  SD <- qnorm(SD, mean = 0, sd = 1) 
  # Exercise 2.2
  pcov <- cor(x = SD[,1], y = SD[,2], method = "pearson")
  kcov <- cor(x = SD[,1], y = SD[,2], method = "kendall")
  scov <- cor(x = SD[,1], y = SD[,2], method = "spearman")
  
  correlation$Pearson <- append(correlation$Pearson, pcov)
  correlation$Kendall <- append(correlation$Kendall, kcov)
  correlation$Spearman <- append(correlation$Spearman, scov)
}
print(dim(SD))

# Exercise 2.3 and 2.4

par(mfrow = c(1,3), mar = c(15,2,3,2))
plot(beta, correlation$Pearson, main = "Pearson Correlation",
     col = "red", ylab = "Cor. value" )
abline(h = 0.8, col = "grey")
plot(beta, correlation$Kendall, main = "Kendall Correlation",
     col = "blue", ylab = "Cor. value")
abline(h = 0.8, col = "grey")
plot(beta, correlation$Spearman, main = "Spearman Correlation", 
     col = "green", ylab = "Cor. value")
abline(h = 0.8, col = "grey")
```

Percebemos pelo gráfico que a correlação é nula para todos quando $\beta = 1$. Além disso, nos três gráficos, observamos que $\beta > 5$ fez com que a correlação superasse $0.8$.

Isso nos mostra que a cópula de Gumbel é muito favorável a variáveis que apresentem alta correlação, dado que o parâmetro $\beta$ permite esse tipo de análise. Os gráficos também são côncavos, o que mostra que o crescimento acentuado do inínio do gráfico vai se reduzindo ao longo da curva. 

```{r message = FALSE}
# Exercise 2.5: Cauchy Distribution
beta <- seq(from = 1, to = 20, by = 0.5)

correlation <- list("Pearson" = c(), "Kendall" = c(), "Spearman" = c())
for (param in beta){
  gcopula <- gumbelCopula(param = param)
  SD <- rCopula(gcopula, n = 2000)
  SD <- qcauchy(SD, location = 0, scale = 1) 

  pcov <- cor(x = SD[,1], y = SD[,2], method = "pearson")
  kcov <- cor(x = SD[,1], y = SD[,2], method = "kendall")
  scov <- cor(x = SD[,1], y = SD[,2], method = "spearman")
  
  correlation$Pearson <- append(correlation$Pearson, pcov)
  correlation$Kendall <- append(correlation$Kendall, kcov)
  correlation$Spearman <- append(correlation$Spearman, scov)
}
print(dim(SD))

par(mfrow = c(1,3), mar = c(15,2,3,2))
plot(beta, correlation$Pearson, main = "Pearson Correlation",
     col = "red", ylab = "Cor. value" )
abline(h = 0.8, col = "grey")
plot(beta, correlation$Kendall, main = "Kendall Correlation",
     col = "blue", ylab = "Cor. value")
abline(h = 0.8, col = "grey")
plot(beta, correlation$Spearman, main = "Spearman Correlation", 
     col = "green", ylab = "Cor. value")
abline(h = 0.8, col = "grey")
```

Da mesma forma como falamos no primeiro exercício, a correlação de Pearson perde o padrão dada a diferença das caudas. Além disso, ela não segue nem o padrão de crescimento antes visto e perde a interpretabilidade. Já a correlação de Kendall e Spearman apresentam comportamento similar, já que procuram reduzir a influência do tamanho das variáveis em questão. 

\pagebreak 

## Análise da ETTJ Brasileira

Primeiro farei o import dos dados. Veja a curva de juros brasileira com as diferentes maturidades especificadas e como se comportou ao longo do tempo. 

```{r message = FALSE}
dir <- "/home/lucasmoschen/Documents/GitHub/Quantitative_Finance/data/"
file <- "prices_Swap_PRE_DI.RData"
load(file = paste(dir,file,sep=""))
yield <- prices_Swap_PRE_DI[,c(1,3,5,6,7,8,9,10,11,2,4)]

timelabels <- c(2002,2005,2008,2011,2014,2016)
X = colnames(yield)[2:length(yield[1,])]
matplot(y = yield[,2:length(yield[1,])], 
        typ="l", lty = 1, xlab = "Days", ylab = "Yield", xaxt = "n",
        main = "Estrutura da Taxa de Juros Brasileira")
axis(1, at = c(0, 650, 1300, 1950,2600,3250), labels = timelabels)
legend("topright", legend = X, lwd = 2, col=1:length(X), cex=0.6)
```

Assim, podemos utilizar a função utilizada para o método PCA. Eu insiro logo abaixo o sumário das informações mais importantes do método.

```{r}
yield.pca = princomp(yield[,2:length(yield)])
summary(yield.pca)

plot(yield.pca,
  main="Proporção da Variância explicada pelos componentes")
```

Como observamos no gráfico, os três primeiros componentes explicam mais de 99,9% da informação que carregamos nos dados. Assim, podemos reduzir a dimensionalidade de 10 para 3 e garantir que, segundo a norma quadrática, tenhamos apenas perdido 0,1% da informação. 

Assim, vejamos as cargas dos componentes de 1 a 4 de nosso algorirmo.

```{r}
par(mar=c(2,4,1,2), mfrow=c(2,2))

time <- c(1,2,3,4,6,12,24,36,48,60)

plot(time,yield.pca$loadings[,1], type = "o", 
     xlab = "months", ylab = "Loading 1", ylim = c(-0.5, 0.5))
plot(time,yield.pca$loadings[,2],  type = "o", 
     xlab = "months", ylab = "Loading 2")
plot(time,yield.pca$loadings[,3],  type = "o", 
     xlab = "months", ylab = "Loading 3")
plot(time,yield.pca$loadings[,4], type = "o", 
     xlab = "months", ylab = "Loading 4")
```

Temos que o primeiro autovetor é praticamente contante, logo ele representa a média das curvas dentre as maturidades. Claro que o valor em si é normalizado, para evitar problemas de variância. A segunda componente mede a tendência, a inclinação das curvas. Percebemos que ela dimminui com o aumento da maturação. O terceiro autovetor captura a curvatura da curva de juros. Percebemos que as curvas de médio prazo tem formato côncavo, por exemplo. Já do quarto componente em diante, não podemos inferir informação, já que elas carregam parte do ruído, pois como já discutido, representam pouco da informação da variância dos dados. 

Podemos também fazer o mesmo processo para quando estamos trabalhando com a diferença diária da curva de juros, como o livro constrói na curva de SWAP. Observe que o gráfico reflete as grandes variações e fica bem complicado de entender o que de fato é relevante na informação.

```{r message = FALSE}
diff_yield <- prices_Swap_PRE_DI[,c(3,5,6,7,8,9,10,11,2,4)]
diff_yield <- diff(as.matrix(diff_yield))

timelabels <- c(2002,2005,2008,2011,2014,2016)
X = colnames(diff_yield)
matplot(y = diff_yield, 
        typ="l", lty = 1, xlab = "Days", ylab = "Yield", xaxt = "n",
        main = "ETTB - Mudanças Diárias")
axis(1, at = c(0, 650, 1300, 1950,2600,3250), labels = timelabels)
legend("topright", legend = X, lwd = 2, col=1:length(X), cex=0.6)
```

Aplicando novamente o algoritmo de PCA, obtemos o sumário das informações e a relação com relação a explicabilidade de cada componente para as diferentes maturidades. Podemos observar que as 5 primeiras componentes explicam mais de 90% da variância. Para além disso, as três primeiras componentes agora explicam quase 80% da informação contida. Também vejamos, logo abaixo o gráfico dos autovetores do método. A interpretação ficou um pouco diferente, pois a primeira componente não é mais uma reta.

```{r fig1, fig.height = 4, fig.width = 8}
diff_yield.pca = princomp(diff_yield)
summary(diff_yield.pca)

par(mar=c(2,4,2,2))
plot(diff_yield.pca,
  main="Proporção da Variância explicada pelos componentes")

par(mar=c(2,4,1,2), mfrow=c(2,2))

time <- c(1,2,3,4,6,12,24,36,48,60)

plot(time,diff_yield.pca$loadings[,1], type = "o", 
     xlab = "months", ylab = "Loading 1", ylim = c(-0.5, 0.5))
plot(time,diff_yield.pca$loadings[,2],  type = "o", 
     xlab = "months", ylab = "Loading 2")
plot(time,diff_yield.pca$loadings[,3],  type = "o", 
     xlab = "months", ylab = "Loading 3")
plot(time,diff_yield.pca$loadings[,4], type = "o", 
     xlab = "months", ylab = "Loading 4")
```

