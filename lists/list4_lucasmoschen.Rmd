---
title: "Finanças Quantitativas: Lista 4"
author: "Lucas Moschen"
affiliation: "Fundação Getulio Vargas"
date: "08 de maio de 2020"
output: pdf_document
---

## Problema 3.17

1. Sejam $Z \sim N(0,1)$, $\sigma > 0$ e $f$ uma função. Então: 
$$
\begin{split}
\mathbb{E}[f(Z)e^{\sigma Z}] &= \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}f(z)e^{\sigma z} e^{-\frac{z^2}{2}} dz \\
&= \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}f(z) e^{-\frac{z^2}{2} + \sigma z} dz \\
&= \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}f(z) e^{-\frac{1}{2}(z^2 - 2\sigma z + \sigma^2)}e^{\frac{1}{2}\sigma^2} dz \\
&= \frac{1}{\sqrt{2\pi}}e^{\frac{1}{2}\sigma^2}\int_{-\infty}^{\infty}f(z) e^{-\frac{1}{2}(z - \sigma)^2} dz, ~~~x = z - \sigma, dx = dz  \\
&= \frac{1}{\sqrt{2\pi}}e^{\frac{1}{2}\sigma^2}\int_{-\infty}^{\infty}f(x + \sigma) e^{-\frac{1}{2}x^2} dx \\
&= e^{\frac{1}{2}\sigma^2}\mathbb{E}[f(Z + \sigma)] \\
\end{split}
$$
Se $X \sim N(\mu, \sigma^2)$, temos que $X = \mu + \sigma Z$, onde $Z \sim N(0,1)$. Se tomarmos $f(Z) = e^{\mu}$ e $sigma > 0$ o desvio vadrão de $X$. Assim: 
$$
\begin{split}
\mathbb{E}[e^X] &= \mathbb{E}[e^{\mu + \sigma Z}] \\
&= \mathbb{E}[e^{\mu}e^{\sigma Z}], ~pelo~demonstrado \\
&= e^{\frac{1}{2}\sigma^2}\mathbb{E}[f(Z + \sigma)] \\
&= e^{\frac{1}{2}\sigma^2}\mathbb{E}[e^{\mu}] = e^{\mu + \sigma^2/2} \\
\end{split}
$$
2. Assuma que $X$ e $Y$ são v.a. juntamente Gaussianas com média $0$ e $h$  uma função qualquer.

Temos que $f_{XY}(x,y)=\frac{1}{2 \pi \sigma_X \sigma_Y \sqrt{1-\rho^2}} \exp \{-\frac{1}{2 (1-\rho^2)} \big[ \frac{x^2}{\sigma_X^2}-2\rho\frac{xy}{\sigma_X\sigma_Y} +\frac{y^2}{\sigma_Y^2} \big] \}$
$$
\begin{split}
\mathbb{E}[e^Xh(Y)] &= \int_{\mathbb{R}^2} e^xh(y)f_{XY}(x,y)dydx \\
&= \int_{\mathbb{R}} e^x\frac{1}{\sqrt{2\pi}\sigma_X}\exp\{-\frac{x^2}{2\sigma_X^2}\} \int_{\mathbb{R}} h(y)\frac{1}{\sqrt{2 \pi} \sigma_Y \sqrt{1-\rho^2}}\exp\{\frac{x^2}{2\sigma_X^2}\} \exp \{-\frac{1}{2 (1-\rho^2)} \big[ \frac{x^2}{\sigma_X^2}-2\rho\frac{xy}{\sigma_X\sigma_Y} +\frac{y^2}{\sigma_Y^2} \big] \} dy dx \\
&= \int_{\mathbb{R}} e^x\frac{1}{\sqrt{2\pi}\sigma_X}\exp\{-\frac{x^2}{2\sigma_X^2}\} \int_{\mathbb{R}} h(y)\frac{1}{\sqrt{2 \pi} \sigma_Y \sqrt{1-\rho^2}}\exp\{ -\frac{1}{2 (1-\rho^2)} \big[\frac{x^2\rho^2}{\sigma_X^2} -2\rho\frac{xy}{\sigma_X\sigma_Y} +\frac{y^2}{\sigma_Y^2} \big] \} dy dx \\
&= \int_{\mathbb{R}} e^x\frac{1}{\sqrt{2\pi}\sigma_X}\exp\{-\frac{x^2}{2\sigma_X^2}\} \int_{\mathbb{R}} h(y)\frac{1}{\sqrt{1-\rho^2}\sqrt{2 \pi} \sigma_Y }\exp\{ -\frac{1}{2 (1-\rho^2)} \left(\frac{y - \frac{x\rho\sigma_Y}{\sigma_X}}{\sigma_Y}\right)^2\} dy dx \\
&= \mathbb{E}[e^X]\mathbb{E}[h(Y + cov(X,Y))] 
\end{split}
$$

## Problema 3.18

1. $\log X \sim N(0, 1)$ e $\log X \sim N(0, \sigma^2)$. Além, $(\log X, \log Y)$ são juntamente Gaussianas. Compute a denside de $X$ quando $\log X \sim N(\mu, \sigma^2)$

Seja $Y = \log X$. Considere $g(y) = e^y$, então $g$ transforma a variável $Y$ na variável $X$. Desta forma, consigo calcular a densidade de $X$: 
$$
\begin{split}
f_X(x) &= \frac{f_Y(y)}{g'(y)} = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y - \mu)^2}{\sigma^2}}\frac{1}{e^y} \\
&= \frac{1}{\sqrt{2\pi}\sigma}\exp\{-\frac{(\log x - \mu)^2}{\sigma^2} - \log x\}
\end{split}
$$
Temos, então, a densidade de $X$.

2. $\rho_{\min} = (e^{-\sigma} - 1)/\sqrt{(e - 1)(e^{\sigma^2} - 1)}$

Pelo descrito no exercício anterior, sabemos que: 

$$E[X] = E[e^{\log X}] = e^{\mu + \sigma^2/2} = e^{1/2}$$
$$E[Y] = e^{\sigma^2/2}$$
$$E[X^2] = E[e^{2\log X}] = e^{2\mu + 2\sigma^2} = e^2$$
$$E[Y^2] = e^{2\sigma^2}$$
$$Var[X] = E[X^2] - E[X]^2 = e^2 - e$$
$$Var[Y] = E[Y^2] - E[Y]^2 = e^{\sigma^2}(e^{\sigma^2} - 1)$$

Podemos dizer que $X$ e $Y$ são contramonotônicas, pois $(X, Y) = (e^{\log X})(e^{-\log Y}) = (e^{\log X})(e^{-\sigma\log X})$, isto é, existe uma variável aleatória e duas funções, uma crescente e uma deecrescente, onde vale essa igualdade em distribuição. Logo, elas admitem o limite inferior Fréchet-Hoeffding na cópula, que no caso é de uma Gaussiana. Assim, o valor mínimo da correlação é atingido quando a cópula é minima, que ocorre quando as variáveis são contramonotônicas. 

Agora, podemos calcular os valores desejados: 
$$
\begin{split}
\rho_{\min} &= \min \frac{Cor(X,Y)}{\sigma_X\sigma_Y} 
= \frac{Cov(e^{\log X}, e^{-\sigma\log X})}{\sigma_X\sigma_Y} \\
&= \frac{E[e^{\log X - \sigma\log X}] - e^{1/2 + \sigma^2/2}}{\sqrt{e(e - 1)}\sqrt{e^{\sigma^2}(e^{\sigma^2} - 1)}} 
= \frac{e^{(-\sigma + 1)^2/2} - e^{(\sigma^2 + 1)/2}}{e^{1/2 + \sigma^2/2}\sqrt{(e - 1)(e^{\sigma^2} - 1)}} \\
&= \frac{e^{(-\sigma + 1)^2/2 - (\sigma^2 + 1)/2} - 1}{\sqrt{(e - 1)(e^{\sigma^2} - 1)}} 
= \frac{e^{-\sigma} - 1}{\sqrt{(e - 1)(e^{\sigma^2} - 1)}}
\end{split}
$$
Como queríamos mostrar. 

3. $\rho_{\max} = (e^{\sigma} - 1)/\sqrt{(e - 1)(e^{\sigma^2} - 1)}$

De forma equivalente, podemos dizer que $X$ e $Y$ são comonotônicas, porque $(X,Y) = (e^{\log X}, e^{\sigma \log X})$ e temos duas funções crescentes com uma variável em comum. Desta forma, elas admitem o limite superior de Fréchet-Hoeffding. DEsta forma, maximizamos a covariância no valor máximo que a cópula assume. Utilizamos as contas do exercício anterior (3.17) e, da mesma forma que fizemos em 2. dessa exercício, a correlação máxima acontece quando as variáveis são comonotônicas. Logo:
$$
\begin{split}
\rho_{\max} &= \max \frac{Cor(X,Y)}{\sigma_X\sigma_Y} 
= \frac{Cov(e^{\log X}, e^{\sigma\log X})}{\sigma_X\sigma_Y} \\
&= \frac{E[e^{\log X + \sigma\log X}] - e^{1/2 + \sigma^2/2}}{\sqrt{e(e - 1)}\sqrt{e^{\sigma^2}(e^{\sigma^2} - 1)}} 
= \frac{e^{(\sigma + 1)^2/2} - e^{(\sigma^2 + 1)/2}}{e^{1/2 + \sigma^2/2}\sqrt{(e - 1)(e^{\sigma^2} - 1)}} \\
&= \frac{e^{(\sigma + 1)^2/2 - (\sigma^2 + 1)/2} - 1}{\sqrt{(e - 1)(e^{\sigma^2} - 1)}} 
= \frac{e^{\sigma} - 1}{\sqrt{(e - 1)(e^{\sigma^2} - 1)}}
\end{split}
$$
Como queríamos mostrar. 

4. $\lim_{\sigma\to\infty} \rho_{\min} = \lim_{\sigma\to\infty} \rho_{\max} = 0$

Como $\sigma > 0$, vemos que $\lim_{\sigma \to \infty} e^{-\sigma} = 0$, e que $e^{\sigma^2} \to \infty$. Dessa maneira, pelo denominador ser ilimitado e o numerador limitado, temos que, de fato, $\lim_{\sigma\to\infty} \rho_{\min} = 0$. 

Para conferir que $\lim_{\sigma\to\infty} \rho_{\max} = 0$, vemos que:

$$
\begin{split}
0 \leq \frac{e^{\sigma} - 1}{((e - 1)(e^{\sigma^2} - 1))^\frac{1}{2}} &\leq \frac{e^{\sigma}}{(e^{\sigma^2} - 1)^\frac{1}{2}}  \\
&= \frac{1}{(e^{\sigma^2} - 1)^\frac{1}{2}/(e^{2\sigma})^{\frac{1}{2}}} \\
&= \frac{1}{(e^{\sigma^2 - 2\sigma} - e^{-2\sigma})^\frac{1}{2}} \\
&\to 0
\end{split}
$$
Isso ocorre porque $e^{\sigma^2 - 2\sigma} \to \infty$, logo o denominador é ilimitado e o numerador limitado. 

Concluo que $\lim_{\sigma\to\infty} \rho_{\min} = \lim_{\sigma\to\infty} \rho_{\max} = 0$.

## Problema 3.24

1. 

```{r message = FALSE}
library(copula)   #carregando a copula
rho <- seq(from = 0, to = 1, by = 0.05)

correlation <- list("Pearson" = c(), "Kendall" = c(), "Spearman" = c())
for (param in rho){
  ncopula <- normalCopula(param = param)
  SD <- rCopula(ncopula, n = 2000)
  # Exercise 1.1
  SD <- qnorm(SD, mean = 0, sd = 1) 
  # Exercise 1.2
  pcov <- cor(x = SD[,1], y = SD[,2], method = "pearson")
  kcov <- cor(x = SD[,1], y = SD[,2], method = "kendall")
  scov <- cor(x = SD[,1], y = SD[,2], method = "spearman")
  
  correlation$Pearson <- append(correlation$Pearson, pcov)
  correlation$Kendall <- append(correlation$Kendall, kcov)
  correlation$Spearman <- append(correlation$Spearman, scov)
}
print(dim(SD))

# Exercise 1.3 and 1.4

par(mfrow = c(3,1), mar = c(1,2,3,2))
plot(rho, correlation$Pearson, main = "Pearson Correlation",
     col = "red", ylab = "Cor. value" )
points(x = rho, y =  rho, type= 'l', col = "grey")
plot(rho, correlation$Kendall, main = "Kendall Correlation",
     col = "blue", ylab = "Cor. value")
points(x = rho, y =  rho, type= 'l', col = "grey")
plot(rho, correlation$Spearman, main = "Spearman Correlation", 
     col = "green", ylab = "Cor. value")
points(x = rho, y =  rho, type= 'l', col = "grey")
```

Observamos que os gráficos de Pearson e Spearman, a correlação das variáveis comparado com a correlação do parâmetro da cópula Gaussiana está em cima da reta $y = x$. Dessa maneira, podemos, pelo menos visualmente, ver que o parâmetro da cópula é exatamente a correlação das variáveis gaussianas que estamos modelando. Isso nos dá uma interpretação bem clara do que é esse parâmetro quando estamos calculando amostras aleatórias.

No caso da correlação de Kendall, vemos que o gráfico fica um pouco a baixo da reta $y = x$, significando que ela é menor do que a correlação de pearson para esses valores. Isso faz sentido por que a contribuição para uma mudança de sinal é equivalente sempre, enquanto na de Pearson, ela é Ponderada por seu tamanho.

```{r message = FALSE}
# Exercise 1.5: Cauchy Distribution
rho <- seq(from = 0, to = 1, by = 0.05)

correlation <- list("Pearson" = c(), "Kendall" = c(), "Spearman" = c())
for (param in rho){
  ncopula <- normalCopula(param = param)
  SD <- rCopula(ncopula, n = 2000)
  SD <- qcauchy(SD, location = 0, scale = 1) 
  pcov <- cor(x = SD[,1], y = SD[,2], method = "pearson")
  kcov <- cor(x = SD[,1], y = SD[,2], method = "kendall")
  scov <- cor(x = SD[,1], y = SD[,2], method = "spearman")
  
  correlation$Pearson <- append(correlation$Pearson, pcov)
  correlation$Kendall <- append(correlation$Kendall, kcov)
  correlation$Spearman <- append(correlation$Spearman, scov)
}

par(mfrow = c(3,1), mar = c(1,2,3,2))
plot(rho, correlation$Pearson, main = "Pearson Correlation",
     col = "red", ylab = "Cor. value" )
points(x = rho, y =  rho, type= 'l', col = "grey")
plot(rho, correlation$Kendall, main = "Kendall Correlation",
     col = "blue", ylab = "Cor. value")
points(x = rho, y =  rho, type= 'l', col = "grey")
plot(rho, correlation$Spearman, main = "Spearman Correlation", 
     col = "green", ylab = "Cor. value")
points(x = rho, y =  rho, type= 'l', col = "grey")
```

A correlação de Pearson ficou extremamente diferente. Isso faz com que o parâmetro nã odeva ser interpretado como a correlação das variáveis, dado que a cauda das distribuições marginais é mais pesada. A correlação de Kendall manteve a propriedade, dado que, como comentado anteriormente, o que é considerado é apenas o sinal, não o tamanho da diferença. Por fim, a correlação de Spearman mostrou-se ainda ter a mesma interpretação. Isso ocorre, pois ela tem o objetivo de remover o tamanho relativo de $X$ e $Y$ e captura a correlação apenas com o que sobrou da dependência, que nesse caso é uma cópula normal, que como visto acima, tem a mesma interpretação para Pearson e Spearman. 

2. 

```{r message = FALSE}
beta <- seq(from = 1, to = 20, by = 0.5)

correlation <- list("Pearson" = c(), "Kendall" = c(), "Spearman" = c())
for (param in beta){
  gcopula <- gumbelCopula(param = param)
  SD <- rCopula(gcopula, n = 2000)
  # Exercise 2.1
  SD <- qnorm(SD, mean = 0, sd = 1) 
  # Exercise 2.2
  pcov <- cor(x = SD[,1], y = SD[,2], method = "pearson")
  kcov <- cor(x = SD[,1], y = SD[,2], method = "kendall")
  scov <- cor(x = SD[,1], y = SD[,2], method = "spearman")
  
  correlation$Pearson <- append(correlation$Pearson, pcov)
  correlation$Kendall <- append(correlation$Kendall, kcov)
  correlation$Spearman <- append(correlation$Spearman, scov)
}
print(dim(SD))

# Exercise 2.3 and 2.4

par(mfrow = c(1,3), mar = c(15,2,3,2))
plot(beta, correlation$Pearson, main = "Pearson Correlation",
     col = "red", ylab = "Cor. value" )
abline(h = 0.8, col = "grey")
plot(beta, correlation$Kendall, main = "Kendall Correlation",
     col = "blue", ylab = "Cor. value")
abline(h = 0.8, col = "grey")
plot(beta, correlation$Spearman, main = "Spearman Correlation", 
     col = "green", ylab = "Cor. value")
abline(h = 0.8, col = "grey")
```

Percebemos pelo gráfico que a correlação é nula para todos quando $\beta = 1$. Além disso, nos três gráficos, observamos que $\beta > 5$ fez com que a correlação superasse $0.8$.

Isso nos mostra que a cópula de Gumbel é muito favorável a variáveis que apresentem alta correlação, dado que o parâmetro $\beta$ permite esse tipo de análise. Os gráficos também são côncavos, o que mostra que o crescimento acentuado do inínio do gráfico vai se reduzindo ao longo da curva. 

```{r message = FALSE}
# Exercise 2.5: Cauchy Distribution
beta <- seq(from = 1, to = 20, by = 0.5)

correlation <- list("Pearson" = c(), "Kendall" = c(), "Spearman" = c())
for (param in beta){
  gcopula <- gumbelCopula(param = param)
  SD <- rCopula(gcopula, n = 2000)
  SD <- qcauchy(SD, location = 0, scale = 1) 

  pcov <- cor(x = SD[,1], y = SD[,2], method = "pearson")
  kcov <- cor(x = SD[,1], y = SD[,2], method = "kendall")
  scov <- cor(x = SD[,1], y = SD[,2], method = "spearman")
  
  correlation$Pearson <- append(correlation$Pearson, pcov)
  correlation$Kendall <- append(correlation$Kendall, kcov)
  correlation$Spearman <- append(correlation$Spearman, scov)
}
print(dim(SD))

par(mfrow = c(1,3), mar = c(15,2,3,2))
plot(beta, correlation$Pearson, main = "Pearson Correlation",
     col = "red", ylab = "Cor. value" )
abline(h = 0.8, col = "grey")
plot(beta, correlation$Kendall, main = "Kendall Correlation",
     col = "blue", ylab = "Cor. value")
abline(h = 0.8, col = "grey")
plot(beta, correlation$Spearman, main = "Spearman Correlation", 
     col = "green", ylab = "Cor. value")
abline(h = 0.8, col = "grey")
```

Da mesma forma como falamos no primeiro exercício, a correlação de Pearson perde o padrão dada a diferença das caudas. Além disso, ela não segue nem o padrão de crescimento antes visto e perde a interpretabilidade. Já a correlação de Kendall e Spearman apresentam comportamento similar, já que procuram reduzir a influência do tamanho das variáveis em questão. 

## Análise da ETTJ Brasileira

Primeiro farei o import dos dados:

```{r}
dir <- "/home/lucasmoschen/Documents/GitHub/Quantitative_Finance/data/"
file <- "prices_Swap_PRE_DI.RData"
load(file = paste(dir,file,sep=""))
```


